Experiment 2: Modified ``d_emb``
--------------------------------

Tokenizers
~~~~~~~~~~

Tokenizers' experiment shared by models.

+------------+------------+----------+-----------+-----------+-------+------------+
| tknzr_name | dset_name  | exp_name | max_vocab | min_count | ver   | is_uncased |
+============+============+==========+===========+===========+=======+============+
| whitespace | wikitext-2 | ws-tknzr | -1        | 10        | train | True       |
+------------+------------+----------+-----------+-----------+-------+------------+

Models Shared Parameters
~~~~~~~~~~~~~~~~~~~~~~~~

+----------------+------------+
| parameters     | value      |
+================+============+
| batch_size     | 32         |
+----------------+------------+
| beta1          | 0.9        |
+----------------+------------+
| beta2          | 0.99       |
+----------------+------------+
| ckpt_step      | 5000       |
+----------------+------------+
| dset_name      | wikitext-2 |
+----------------+------------+
| eps            | 1e-8       |
+----------------+------------+
| log_step       | 2500       |
+----------------+------------+
| lr             | 1e-3       |
+----------------+------------+
| max_norm       | 1          |
+----------------+------------+
| max_seq_len    | 512        |
+----------------+------------+
| n_epoch        | 200        |
+----------------+------------+
| seed           | 42         |
+----------------+------------+
| tknzr_exp_name | ws-tknzr   |
+----------------+------------+
| ver            | train      |
+----------------+------------+
| d_hid          | 300        |
+----------------+------------+
| n_hid_lyr      | 2          |
+----------------+------------+
| n_post_hid_lyr | 2          |
+----------------+------------+
| n_pre_hid_lyr  | 2          |
+----------------+------------+
| p_emb          | 0.1        |
+----------------+------------+
| p_hid          | 0.1        |
+----------------+------------+
| wd             | 1e-2       |
+----------------+------------+

Models Perplexity Performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+-----------------------------------------------------------------------+
| Training Set                                                          |
+-----------+-----------+-----------+-----------+-----------+-----------+
| ``d_emb`` | step 10k  | step 30k  | step 50k  | step 70k  | step 90k  |
+===========+===========+===========+===========+===========+===========+
| 32        | 4.18      | 3.42      | 3.207     | 3.086     | 3.01      |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 112       | 3.708     | 3.134     | 2.936     | 2.814     | 2.717     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 192       | 3.517     | 3         | 2.797     | 2.674     | 2.579     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 272       | 3.458     | 2.944     | 2.743     | 2.624     | 2.526     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 352       | 3.395     | 2.897     | 2.69      | 2.565     | 2.471     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 432       | 3.355     | 2.863     | 2.664     | 2.541     | 2.443     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 512       | **3.328** | **2.848** | **2.636** | **2.512** | **2.416** |
+-----------+-----------+-----------+-----------+-----------+-----------+


+-----------------------------------------------------------------------+
| Validation Set                                                        |
+-----------+-----------+-----------+-----------+-----------+-----------+
| ``d_emb`` | step 10k  | step 30k  | step 50k  | step 70k  | step 90k  |
+===========+===========+===========+===========+===========+===========+
| 32        | 4.054     | 3.691     | 3.629     | 3.603     | 3.588     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 112       | 3.735     | 3.514     | 3.495     | 3.476     | 3.475     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 192       | 3.634     | 3.481     | 3.47      | 3.471     | 3.46      |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 272       | 3.589     | 3.456     | 3.466     | **3.458** | **3.459** |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 352       | 3.559     | **3.443** | **3.454** | 3.473     | 3.483     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 432       | 3.551     | 3.447     | 3.461     | 3.482     | 3.498     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 512       | **3.544** | 3.453     | 3.46      | 3.47      | 3.493     |
+-----------+-----------+-----------+-----------+-----------+-----------+


+-----------------------------------------------------------------------+
| Testing Set                                                           |
+-----------+-----------+-----------+-----------+-----------+-----------+
| ``d_emb`` | step 10k  | step 30k  | step 50k  | step 70k  | step 90k  |
+===========+===========+===========+===========+===========+===========+
| 32        | 4.029     | 3.672     | 3.625     | 3.6       | 3.581     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 112       | 3.714     | 3.494     | 3.49      | 3.479     | **3.471** |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 192       | 3.618     | 3.469     | 3.479     | 3.482     | 3.48      |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 272       | 3.582     | 3.462     | 3.473     | **3.456** | 3.478     |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 352       | 3.552     | **3.437** | **3.447** | 3.471     | 3.49      |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 432       | 3.537     | 3.448     | 3.46      | 3.484     | 3.5       |
+-----------+-----------+-----------+-----------+-----------+-----------+
| 512       | **3.521** | 3.445     | 3.466     | 3.476     | 3.511     |
+-----------+-----------+-----------+-----------+-----------+-----------+


Conclusions
~~~~~~~~~~~

- Perplexity performance on **training set**:

  - ``d_emb=512`` perform the **best** on all steps.
  - ``d_emb=32`` perform the **worst** on all steps.

- Perplexity performance on **validation set**:
  - ``d_emb=512`` perform the **best** on ``10k`` steps.
  - ``d_emb=352`` perform the **best** on ``30k`` and ``50k`` steps.
  - ``d_emb=272`` perform the **best** on ``70k`` and ``90k`` steps.
  - ``d_emb=32`` perform the **worst** on all steps.

- Perplexity performance on **testing set**:

  - ``d_emb=512`` perform the **best** on ``10k`` steps.
  - ``d_emb=352`` perform the **best** on ``30k`` and ``50k`` steps.
  - ``d_emb=272`` perform the **best** on ``70k`` steps steps.
  - ``d_emb=112`` perform the **best** on ``90k`` steps steps.
  - ``d_emb=32`` perform the **worst** on all steps.

- ``d_emb=32`` is **underfitting**.

  - ``d_emb=32`` perform the **worst** on all sets.

- ``d_emb=512`` is **overfitting**.

  - ``d_emb=512`` perform the **best** on **training set**, but not on **validation set** and **testing set**.

