Experiment 4: Modified ``lr``
-----------------------------------------

Tokenizers
~~~~~~~~~~

Tokenizers' experiment shared by models.

+------------+------------+----------+-----------+-----------+-------+------------+
| tknzr_name | dset_name  | exp_name | max_vocab | min_count | ver   | is_uncased |
+============+============+==========+===========+===========+=======+============+
| whitespace | wikitext-2 | ws-tknzr | -1        | 10        | train | True       |
+------------+------------+----------+-----------+-----------+-------+------------+

Models Shared Parameters
~~~~~~~~~~~~~~~~~~~~~~~~

+----------------+------------+
| parameters     | value      |
+================+============+
| batch_size     | 32         |
+----------------+------------+
| beta1          | 0.9        |
+----------------+------------+
| beta2          | 0.99       |
+----------------+------------+
| ckpt_step      | 2500       |
+----------------+------------+
| dset_name      | wikitext-2 |
+----------------+------------+
| eps            | 1e-8       |
+----------------+------------+
| log_step       | 200        |
+----------------+------------+
| max_norm       | 1          |
+----------------+------------+
| max_seq_len    | 512        |
+----------------+------------+
| n_epoch        | 100        |
+----------------+------------+
| seed           | 42         |
+----------------+------------+
| tknzr_exp_name | ws-tknzr   |
+----------------+------------+
| ver            | train      |
+----------------+------------+
| d_emb          | 200        |
+----------------+------------+
| d_hid          | 300        |
+----------------+------------+
| n_hid_lyr      | 2          |
+----------------+------------+
| n_post_hid_lyr | 2          |
+----------------+------------+
| n_pre_hid_lyr  | 2          |
+----------------+------------+
| p_emb          | 0.1        |
+----------------+------------+
| p_hid          | 0.1        |
+----------------+------------+
| wd             | 1e-2       |
+----------------+------------+


Models Loss Performance
~~~~~~~~~~~~~~~~~~~~~~~

+--------+-----------+-----------+-----------+------------+------------+
| ``lr`` | step 8k   | step 10k  | step 15k  | step 30k   | step 50k   |
+========+===========+===========+===========+============+============+
| 1e-1   | 2.037     | 2.014     | 2.059     | 2.054      | 2.005      |
+--------+-----------+-----------+-----------+------------+------------+
| 1e-2   | 1.336     | 1.195     | 1.23      | 1.292      | 1.295      |
+--------+-----------+-----------+-----------+------------+------------+
| 1e-3   | **1.048** | **1.014** | **1.002** | **0.9547** | **0.8965** |
+--------+-----------+-----------+-----------+------------+------------+
| 1e-4   | 1.267     | 1.228     | 1.21      | 1.126      | 1.038      |
+--------+-----------+-----------+-----------+------------+------------+
| 1e-5   | 6.858     | 1.496     | 1.45      | 1.358      | 1.278      |
+--------+-----------+-----------+-----------+------------+------------+


Models Perplexity Performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+--------------------------------------------------------------------+
| Training Set                                                       |
+--------+-----------+-----------+-----------+-----------+-----------+
| ``lr`` | step 10k  | step 15k  | step 30k  | step 40k  | step 50k  |
+========+===========+===========+===========+===========+===========+
| 1e-1   | 22.2      | 24.12     | 22.65     | 20.13     | 22.65     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-2   | 4.763     | 4.913     | 5.441     | 5.738     | 6.106     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-3   | **3.524** | **3.303** | **2.999** | **2.887** | **2.799** |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-4   | 5.315     | 4.857     | 4.162     | 3.914     | 3.739     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-5   | 9.416     | 7.909     | 6.558     | 6.232     | 5.997     |
+--------+-----------+-----------+-----------+-----------+-----------+


+--------------------------------------------------------------------+
| Validation Set                                                     |
+--------+-----------+-----------+-----------+-----------+-----------+
| ``lr`` | step 10k  | step 15k  | step 30k  | step 40k  | step 50k  |
+========+===========+===========+===========+===========+===========+
| 1e-1   | 16.03     | 16.87     | 16.14     | 14.99     | 16.12     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-2   | 4.482     | 4.566     | 4.913     | 5.179     | 5.448     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-3   | **3.643** | **3.552** | **3.471** | **3.464** | **3.468** |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-4   | 4.74      | 4.441     | 4.004     | 3.86      | 3.773     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-5   | 7.584     | 6.558     | 5.592     | 5.362     | 5.199     |
+--------+-----------+-----------+-----------+-----------+-----------+

+--------------------------------------------------------------------+
| Testing Set                                                        |
+--------+-----------+-----------+-----------+-----------+-----------+
| ``lr`` | step 10k  | step 15k  | step 30k  | step 40k  | step 50k  |
+========+===========+===========+===========+===========+===========+
| 1e-1   | 17.74     | 19.12     | 17.96     | 16.4      | 17.98     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-2   | 4.466     | 4.587     | 4.929     | 5.197     | 5.483     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-3   | **3.622** | **3.545** | **3.453** | **3.453** | **3.462** |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-4   | 4.734     | 4.426     | 3.982     | 3.834     | 3.754     |
+--------+-----------+-----------+-----------+-----------+-----------+
| 1e-5   | 7.911     | 6.741     | 5.661     | 5.412     | 5.237     |
+--------+-----------+-----------+-----------+-----------+-----------+


Conclusions
~~~~~~~~~~~
- Loss performance on **training set**:
    - ``lr=1e-1`` perform the **worst** after ``10k`` steps.
    - ``lr=1e-3`` perform the **best** on all steps.
- Perplexity performance on **training set**:
    - ``lr=1e-1`` perform the **worst** on all steps.
    - ``lr=1e-3`` perform the **best** on all steps.
- Perplexity performance on **validation set**:
    - ``lr=1e-1`` perform the **worst** on all steps.
    - ``lr=1e-3`` perform the **best** on all steps.
- Perplexity performance on **testing set**:
    - ``lr=1e-1`` perform the **worst** on all steps.
    - ``lr=1e-3`` perform the **best** on all steps.
- ``lr=1e-1`` is **underfitting**.
    - ``lr=1e-1`` does not **converge** on loss.
    - ``lr=1e-1`` perform the **worst** on all sets.
- ``lr=1e-3`` perform the **best** on **Perplexity** and **Loss**.

