WikiText-2 Dataset
==================

Experiment 1: Models Performance Baseline
-----------------------------------------

Tokenizers
~~~~~~~~~~

Tokenizers' experiment shared by models.

+------------+------------+----------+-----------+-----------+-------+------------+
| tknzr_name | dset_name  | exp_name | max_vocab | min_count | ver   | is_uncased |
+============+============+==========+===========+===========+=======+============+
| whitespace | wikitext-2 | ws-tknzr | -1        | 10        | train | True       |
+------------+------------+----------+-----------+-----------+-------+------------+

Models Shared Parameters
~~~~~~~~~~~~~~~~~~~~~~~~

+----------------+------------+
| parameters     | value      |
+================+============+
| batch_size     | 32         |
+----------------+------------+
| beta1          | 0.9        |
+----------------+------------+
| beta2          | 0.99       |
+----------------+------------+
| ckpt_step      | 10000      |
+----------------+------------+
| dset_name      | wikitext-2 |
+----------------+------------+
| eps            | 1e-8       |
+----------------+------------+
| log_step       | 2500       |
+----------------+------------+
| lr             | 1e-3       |
+----------------+------------+
| max_norm       | 1          |
+----------------+------------+
| max_seq_len    | 512        |
+----------------+------------+
| n_epoch        | 1000       |
+----------------+------------+
| seed           | 42         |
+----------------+------------+
| tknzr_exp_name | ws-tknzr   |
+----------------+------------+
| ver            | train      |
+----------------+------------+
| d_emb          | 100        |
+----------------+------------+
| d_hid          | 300        |
+----------------+------------+
| n_hid_lyr      | 2          |
+----------------+------------+
| n_post_hid_lyr | 2          |
+----------------+------------+
| n_pre_hid_lyr  | 2          |
+----------------+------------+
| p_emb          | 0.1        |
+----------------+------------+
| p_hid          | 0.1        |
+----------------+------------+
| wd             | 1e-2       |
+----------------+------------+

Models Loss Performance
~~~~~~~~~~~~~~~~~~~~~~~

+----------------+------------+------------+------------+-------------+
| model_name     | step 50k   | step 100k  | step 200k  | step 547.5k |
+================+============+============+============+=============+
| RNN            | 1.0840     | 1.0780     | 1.0750     | 1.0690      |
+----------------+------------+------------+------------+-------------+
| GRU            | 0.9661     | 0.9212     | 0.8843     | 0.8586      |
+----------------+------------+------------+------------+-------------+
| LSTM           | **0.9385** | **0.8932** | **0.8586** | **0.8293**  |
+----------------+------------+------------+------------+-------------+
| res-RNN        | 1.0850     | 1.0770     | 1.0640     | 1.0420      |
+----------------+------------+------------+------------+-------------+
| res-GRU        | 0.9660     | 0.9240     | 0.8874     | 0.8624      |
+----------------+------------+------------+------------+-------------+
| res-LSTM       | 0.9540     | 0.9079     | 0.8710     | 0.8451      |
+----------------+------------+------------+------------+-------------+

Models Perplexity Performance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

+--------------------------------------------------------------------+
| Training Set                                                       |
+----------------+------------+------------+------------+------------+
| model_name     | step 10k   | step 100k  | step 200k  | step 540k  |
+================+============+============+============+============+
| RNN            | > 4        | 3.8620     | 3.8200     | 3.7750     |
+----------------+------------+------------+------------+------------+
| GRU            | 3.7940     | 2.8290     | 2.6280     | 2.5040     |
+----------------+------------+------------+------------+------------+
| LSTM           | **3.7050** | **2.6850** | **2.4980** | **2.3690** |
+----------------+------------+------------+------------+------------+
| res-RNN        | > 4        | 3.8160     | 3.7180     | 3.5810     |
+----------------+------------+------------+------------+------------+
| res-GRU        | 3.8030     | 2.8190     | 2.6330     | 2.5120     |
+----------------+------------+------------+------------+------------+
| res-LSTM       | 3.7900     | 2.7490     | 2.5510     | 2.4210     |
+----------------+------------+------------+------------+------------+

+--------------------------------------------------------------------+
| Validation Set                                                     |
+----------------+------------+------------+------------+------------+
| model_name     | step 10k   | step 100k  | step 200k  | step 540k  |
+================+============+============+============+============+
| RNN            | 4.0940     | 3.8720     | 3.9410     | 3.9580     |
+----------------+------------+------------+------------+------------+
| GRU            | 3.7960     | **3.4630** | **3.4400** | **3.4510** |
+----------------+------------+------------+------------+------------+
| LSTM           | **3.7400** | 3.4960     | 3.5010     | 3.5180     |
+----------------+------------+------------+------------+------------+
| res-RNN        | 4.0910     | 3.8720     | 3.8890     | 3.8330     |
+----------------+------------+------------+------------+------------+
| res-GRU        | 3.8080     | 3.4880     | 3.4870     | 3.4750     |
+----------------+------------+------------+------------+------------+
| res-LSTM       | 3.7930     | 3.5240     | 3.5540     | 3.5700     |
+----------------+------------+------------+------------+------------+

+--------------------------------------------------------------------+
| Testing Set                                                        |
+----------------+------------+------------+------------+------------+
| model_name     | step 10k   | step 100k  | step 200k  | step 540k  |
+================+============+============+============+============+
| RNN            | 4.0770     | 3.8730     | 3.9510     | 3.9620     |
+----------------+------------+------------+------------+------------+
| GRU            | 3.7760     | **3.4700** | **3.4610** | **3.4640** |
+----------------+------------+------------+------------+------------+
| LSTM           | **3.7100** | 3.4970     | 3.5140     | 3.5340     |
+----------------+------------+------------+------------+------------+
| res-RNN        | 4.0570     | 3.8830     | 3.9180     | 3.8330     |
+----------------+------------+------------+------------+------------+
| res-GRU        | 3.7860     | 3.4890     | 3.4870     | 3.4940     |
+----------------+------------+------------+------------+------------+
| res-LSTM       | 3.7770     | 3.5350     | 3.5620     | 3.5770     |
+----------------+------------+------------+------------+------------+

Conclusions
~~~~~~~~~~~

- On **training set**
    - **LSTM** perform the best.
    - **RNN** perform the worst.
- On **validation set**
    - **GRU** perform the best.
    - **RNN** perform the worst.
- On **testing set**
    - **GRU** perform the best.
    - **RNN** perform the worst.
- Loss does not **directly** reflect on perplexity.
    - **LSTM** got the best loss but **GRU** got the best perplexity
- All ``*sattn*`` models **do not** perform well under same optimization settings.
- Using residual connections does help on **RNN**, but not on **GRU** and even worse on **LSTM**.
- Loss does not go down much after ``200k`` steps.
