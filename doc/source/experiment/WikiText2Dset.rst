WikiText-2 Dataset
==================

Tokenizers
----------

Tokenizers' experiment shared by models.

+------------+------------+----------+-----------+-----------+-------+------------+
| tknzr_name | dset_name  | exp_name | max_vocab | min_count | ver   | is_uncased |
+============+============+==========+===========+===========+=======+============+
| whitespace | wikitext-2 | ws-tknzr | -1        | 10        | train | True       |
+------------+------------+----------+-----------+-----------+-------+------------+

Performance change when using different models
----------------------------------------------

Shared parameters
~~~~~~~~~~~~~~~~~

+----------------+------------+
| parameters     | value      |
+================+============+
| batch_size     | 32         |
+----------------+------------+
| beta1          | 0.9        |
+----------------+------------+
| beta2          | 0.99       |
+----------------+------------+
| ckpt_step      | 10000      |
+----------------+------------+
| dset_name      | wikitext-2 |
+----------------+------------+
| eps            | 1e-8       |
+----------------+------------+
| log_step       | 2500       |
+----------------+------------+
| lr             | 1e-3       |
+----------------+------------+
| max_norm       | 1          |
+----------------+------------+
| max_seq_len    | 512        |
+----------------+------------+
| n_epoch        | 1000       |
+----------------+------------+
| seed           | 42         |
+----------------+------------+
| tknzr_exp_name | ws-tknzr   |
+----------------+------------+
| ver            | train      |
+----------------+------------+
| d_emb          | 100        |
+----------------+------------+
| d_hid          | 300        |
+----------------+------------+
| n_hid_lyr      | 2          |
+----------------+------------+
| n_post_hid_lyr | 2          |
+----------------+------------+
| n_pre_hid_lyr  | 2          |
+----------------+------------+
| p_emb          | 0.1        |
+----------------+------------+
| p_hid          | 0.1        |
+----------------+------------+
| wd             | 1e-2       |
+----------------+------------+

Loss performance
~~~~~~~~~~~~~~~~

+----------------+---------+
| model_name     | step -1 |
+----------------+---------+
| RNN            | 1.0690  |
+----------------+---------+
| GRU            | 0.8586  |
+----------------+---------+
| LSTM           | N/A     |
+----------------+---------+
| res-RNN        | N/A     |
+----------------+---------+
| res-GRU        | N/A     |
+----------------+---------+
| res-LSTM       | N/A     |
+----------------+---------+
| sattn-RNN      | N/A     |
+----------------+---------+
| sattn-GRU      | N/A     |
+----------------+---------+
| sattn-LSTM     | N/A     |
+----------------+---------+
| res-sattn-RNN  | N/A     |
+----------------+---------+
| res-sattn-GRU  | N/A     |
+----------------+---------+
| res-sattn-LSTM | N/A     |
+----------------+---------+
